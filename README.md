# Documentation

First of all, I want to thank you for the opportunity to solve this take-home task. The whole process of understanding the problem domain, implementing the solution, fine-tuning the parameters, and finally trying to optimize it was really fun! I finished most of the work already on Thursday this week, but the nice weekend delayed my report submission. Sorry for that.

Next, in the following text, I segmented the steps I’ve taken to understand the problem and implement the solution:

## Setups

1. The main branch contains the final solution, images used for the reporting, and final code.
2. The dev branch contains the debug flag. It builds by default with the debug flag.
3. The dev_c2_optl branch contains the optimization work done with Claude. It was done in a separate repo but pushed here for completeness and transparency.

## Steps

1. **Problem understanding**: Learning from function signatures and ChatGPT  

    This step involved understanding the abstract problem: *Given a rectangular area in a picture, detect the biggest shape and blur it*. I used ChatGPT to understand the common approach to solving it. After a few iterations with ChatGPT, the solution workflow visualized as a diagram by ChatGPT looked like [this](image.png).  

    From this diagram, I understood a few key concepts: converting to grayscale, converting to black and white using thresholding, and the need for a mask.  

    Also in this step, when it came to the code, I started by looking at the function signature to understand what data structure was needed:  
    ```c++
    py::array_t<uint8_t> blur_largest_shape_in_rect(
        py::array_t<uint8_t> input_array,
        py::tuple rect_tuple,
        int blur_kernel = 15)
    ```
    Looking at `py::array_t<uint8_t>` as a function return, and `py::array_t<uint8_t> input_array`, I initially thought this was a two-dimensional array by default, and maybe this function only took grayscale images. I even sent an email in a rush to ask Vojta if this function only takes gray images. But very soon I realized it can actually take an n-dimensional array, and the dimensions are stored in `buffer_info`.

    After these two steps, and looking at the generic code given by ChatGPT, it seemed this was not a very complicated problem to solve. I was ready to iteratively implement it with the help of GitHub Copilot in VS Code.  

    At this point, I also realized there were no sample input data or performance requirements given.

2. **DevOps and Linter Setup**  
    Before implementing anything, I needed the setup to work in the container to ensure a smoother development experience and good practices:  
    - Set up a devcontainer based on Dockerfile, including configuring the correct paths for packages.  
    - Set up linter behavior in `.vscode/settings.json`.

3. **Iterated implementation with Copilot**  
    Next, I iteratively implemented the solution in `run_pipeline.py` and `cpp_module/cpp_module`. In Python, since there were requirements for input parameters and many conditional checks were needed for input sanity, I introduced `Pydantic` to handle input validation clearly.  

    After the Python implementation, I added a few commented lines in the C++ file based on the flowchart generated by ChatGPT earlier, and implemented it using Copilot.  

    The whole implementation was rather fast. I had v0 of the code.

4. **Understanding the code and cross-checking correctness**  
    To understand the code given by Copilot (which already looked very similar to ChatGPT’s), I researched OpenCV packages to understand the general approach to `edge detection`. [This article](https://opencv.org/blog/edge-detection-using-opencv/) helped the most.  

    This step also increased my understanding of a few concepts (see the Appendix below).  

    It became clear that inspecting intermediate results was needed to judge how well the pipeline was performing, by checking the following images:  
    - ROI: to check if the right area is selected.  
    - Gray: the grayscale image.  
    - Edges: to see what contours are detected.  
    - Mask: the final shape to be blurred.

4. **Pulling sample data, testing pipeline, and parameter tuning**  
    At this point, I needed to pull some sample data for testing the pipeline. Given Egonym works with facial data, I pulled a portrait from the internet — the “smiling lady” image. This portrait later helped with parameter tuning.  

    Inspecting the result for the first [sample data](./input_images_test/1/smiling_lady.jpg), I saw it blurred too much area. The issue was an ill-generated mask. **It turns out tuning the mask was not as easy as it appears** — the detected edges through Canny directly influence how the mask will look. Clearer pictures make edge detection easier. For the sample data, the edges looked like [this](./report/smiling_lady_edges.jpg) and the mask had only one contour [(seen here)](./report/smiling_lady_mask.jpg).  

    When I switched to a [standard LinkedIn portrait](./input_images_test/2/better_protrait_rect160_1_200_240.jpg), the edge picture was [this one](./output_images_test/report/better_protrait_rect160_1_200_240_edges.jpg) and the mask was [much better](./output_images_test/report/better_protrait_rect160_1_200_240_mask.jpg) — just missing one closing line.  

    I then tuned:  
    - The hysteresis thresholds of the Canny function.  
    - The blur kernel in the first Gaussian Blur function.  

    This improved results: [more detailed edges](./report/tunning/better_protrait_rect160_1_200_240_edges.jpg) and [a closed mask](./report/tunning/better_protrait_rect160_1_200_240_mask.jpg).  

    ChatGPT suggested applying dilation or erosion. After trial and error, the best mask came from using:  
    ```c++
    cv::morphologyEx(edges, edges, cv::MORPH_CLOSE, ...);
    ```
    This connected contours and produced [good edges](./output_images_test/better_protrait_edges.jpg), [a mask matching the head](./output_images_test/better_protrait_mask.jpg), and [a good blur](./output_images_test/better_protrait_rect160_1_200_240.jpg).  

    I repeated tuning on other images, learning parameter effects — e.g., for the [5th picture](./input_images_test/5/mens_gromming_rect100_1_400_300.jpg), where face–clothing contrast is high, using very high thresholds gave the best result [as seen here](./output_images_test/mens_gromming_mask.jpg).  

    After this step, the pipeline compiled and built successfully.

5. **Implementing another solution using Claude Code CLI**  
    For comparison, I created another repo where Claude CLI implemented the whole solution from scratch. The code was not meaningfully different from mine, so I did not include it here.

6. **Performance improvement**  
    Though not required, I did three iterations with Claude Code CLI for fun in a separate repo:  
    - Ask Claude to identify performance issues.  
    - Add runtime profiling under a debug flag in both C++ and Python.  
    - Consult a performance optimization [tutorial](https://www.opencvhelp.org/tutorials/best-practices/performance-optimization/) for OpenCV.  
    - Integrate advice from the tutorial.  

    Using `cv::UMat` initially showed little benefit — likely due to CPU–GPU data copying and because I hadn’t installed Nvidia drivers for my graphics card. After installing them, I commissioned Claude to produce a final optimal solution report.  

    Since optimization is optional, I didn't modify the code in the main branch to the optimized version, but rather kept it on a separate branch `dev_c2_optl`. But I included the three reports in the main branch: [v0](./report/PERFORMANCE_OPTIMIZATION_REPORT.md), [v1](./report/PERFORMANCE_OPTIMIZATION_REPORT_V2.md), and [final](./report/FINAL_GPU_PERFORMANCE_REPORT.md).

---

### Appendix  
> 1. "Color images are often converted to grayscale before applying edge detection techniques like Canny, Sobel, or Laplacian. This simplifies the process by reducing the image to a single intensity channel, making it easier to detect edges based on intensity changes."  

> 2. "After processing with Sobel detector, since the result is in floating-point format, it’s converted to an 8-bit unsigned format using `convertScaleAbs()` so it can be displayed or saved properly."  

> 3. The concept of hysteresis: If the gradient magnitude value is higher than the larger threshold, those pixels are solid edges and included in the final edge map. If lower than the smaller threshold, pixels are suppressed. Pixels in between are “weak” edges — included only if connected to strong edges.  
